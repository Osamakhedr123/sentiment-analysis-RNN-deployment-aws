{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a Sentiment Analysis myb App\r\n",
    "## Using PyTorch and SageMaker\r\n",
    "\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "My goal in this project will be to have a simple web page which a user can use to enter a movie review. The web page will then send the review off to the deployed model which will predict the sentiment of the entered review.\r\n",
    "\r\n",
    "## General Outline\r\n",
    "\r\n",
    "1. Download or otherwise retrieve the data.\r\n",
    "2. Process / Prepare the data.\r\n",
    "3. Upload the processed data to S3.\r\n",
    "4. Train a chosen model.\r\n",
    "5. Test the trained model (typically using a batch transform job).\r\n",
    "6. Deploy the trained model.\r\n",
    "7. Use the deployed model.\r\n",
    "\r\n",
    "For this project, I will be following the steps in the general outline with some modifications. \r\n",
    "\r\n",
    "First, I will not be testing the model in its own step. I will still be testing the model, however, I will do it by deploying my model and then using the deployed model by sending the test data to it. One of the reasons for doing this is so that I can make sure that my deployed model is working correctly before moving forward.\r\n",
    "\r\n",
    "In addition, I will deploy and use my trained model a second time. In the second iteration you will customize the way that my trained model is deployed by including some of my own code. In addition, my newly deployed model will be used in the sentiment analysis web app."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# make sure that SageMaker 1.x is used\r\n",
    "!pip install sagemaker==1.72.0"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: sagemaker==1.72.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.72.0)\n",
      "Requirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.18.28)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.17.2)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (20.9)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.5.3)\n",
      "Requirement already satisfied: smdebug-rulesconfig==0.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (4.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.22.0,>=1.21.28 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.21.28)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.5.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.22.0,>=1.21.28->boto3>=1.14.12->sagemaker==1.72.0) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.22.0,>=1.21.28->boto3>=1.14.12->sagemaker==1.72.0) (1.26.5)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.72.0) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Downloading the data\r\n",
    "\r\n",
    "For this notebook, I will be using the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/)\r\n",
    "\r\n",
    "> Maas, Andrew L., et al. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/). In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_. Association for Computational Linguistics, 2011."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%mkdir ../data\r\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\r\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir: cannot create directory ‘../data’: File exists\n",
      "--2021-09-03 16:10:56--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  25.4MB/s    in 3.7s    \n",
      "\n",
      "2021-09-03 16:11:00 (21.4 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Preparing and Processing the data\r\n",
    "\r\n",
    "Also, as in the XGBoost notebook, I will be doing some initial data processing. To begin with, I will read in each of the reviews and combine them into a single input structure. Then, I will split the dataset into a training set and a testing set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\r\n",
    "import glob\r\n",
    "\r\n",
    "def read_imdb_data(data_dir='../data/aclImdb'):\r\n",
    "    data = {}\r\n",
    "    labels = {}\r\n",
    "    \r\n",
    "    for data_type in ['train', 'test']:\r\n",
    "        data[data_type] = {}\r\n",
    "        labels[data_type] = {}\r\n",
    "        \r\n",
    "        for sentiment in ['pos', 'neg']:\r\n",
    "            data[data_type][sentiment] = []\r\n",
    "            labels[data_type][sentiment] = []\r\n",
    "            \r\n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\r\n",
    "            files = glob.glob(path)\r\n",
    "            \r\n",
    "            for f in files:\r\n",
    "                with open(f) as review:\r\n",
    "                    data[data_type][sentiment].append(review.read())\r\n",
    "                    # here I represent a positive review by '1' and a negative review by '0'\r\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\r\n",
    "                    \r\n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\r\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\r\n",
    "                \r\n",
    "    return data, labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data, labels = read_imdb_data()\r\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\r\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\r\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that I've read the raw training and testing data from the downloaded dataset, I will combine the positive and negative reviews and shuffle the resulting records."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from sklearn.utils import shuffle\r\n",
    "\r\n",
    "def prepare_imdb_data(data, labels):\r\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\r\n",
    "    \r\n",
    "    # combine positive and negative reviews and labels\r\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\r\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\r\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\r\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\r\n",
    "    \r\n",
    "    # shuffle reviews and corresponding labels within training and test sets\r\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\r\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\r\n",
    "    \r\n",
    "    # return a unified training data, test data, training labels, test labets\r\n",
    "    return data_train, data_test, labels_train, labels_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\r\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that I have our training and testing sets unified and prepared, I should do a quick check and see an example of the data THE model will be trained on. This is generally a good idea as it allows us to see how each of the further processing steps affects the reviews and it also ensures that the data has been loaded correctly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(train_X[100])\r\n",
    "print(train_y[100])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"Scoop\" is also the name of a late-Thirties Evelyn Waugh novel, and Woody Allen's new movie, though set today, has a nostalgic charm and simplicity. It hasn't the depth of characterization, intense performances, suspense or shocking final frisson of Allen's penultimate effort \"Match Point,\" (argued by many, including this reviewer, to be a strong return to form) but \"Scoop\" does closely resemble Allen's last outing in its focus on English aristocrats, posh London flats, murder, and detection. This time Woody leaves behind the arriviste murder mystery genre and returns to comedy, and is himself back on the screen as an amiable vaudevillian, a magician called Sid Waterman, stage moniker The Great Splendini, who counters some snobs' probing with, \"I used to be of the Hebrew persuasion, but as I got older, I converted to narcissism.\" Following a revelation in the midst of Splendini's standard dematerializing act, with Scarlett Johansson (as Sondra Pransky) the audience volunteer, the mismatched pair get drawn into a dead ace English journalist's post-mortem attempt to score one last top news story. On the edge of the Styx Joe Strombel (Ian McShane) has just met the shade of one Lord Lyman's son's secretary, who says she was poisoned, and she's told him the charming aristocratic bounder son Peter Lyman (Hugh Jackman) was the Tarot Card murderer, a London serial killer. Sondra and Sid immediately become a pair of amateur sleuths. With Sid's deadpan wit and Sondra's bumptious beauty they cut a quick swath through to the cream of the London aristocracy.<br /><br />Woody isn't pawing his young heroine muse -- as in \"Match Point,\" Johansson again -- as in the past. This time moreover Scarlett's not an ambitious sexpot and would-be movie star. She's morphed surprisingly into a klutzy, bespectacled but still pretty coed. Sid and Sondra have no flirtation, which is a great relief. They simply team up, more or less politely, to carry out Strombel's wishes by befriending Lyman and watching him for clues to his guilt. With only minimal protests Sid consents to appear as Sondra's dad. Sondra, who's captivated Peter by pretending to drown in his club pool, re-christens herself Jade Spence. Mr. Spence, i.e., Woody, keeps breaking cover by doing card tricks, but he amuses dowagers with these and beats their husbands at poker, spewing non-stop one-liners and all the while maintaining, apparently with success, that he's in oil and precious metals, just as \"Jade\" has told him to say.<br /><br />That's about all there is to it, or all that can be told without spoiling the story by revealing its outcome. At first Allen's decision to make Johansson a gauche, naively plainspoken, and badly dressed college girl seems not just unkind but an all-around bad decision. But Johansson, who has pluck and panache as an actress, miraculously manages to carry it off, helped by Jackman, an actor who knows how to make any actress appear desirable, if he desires her. The film actually creates a sense of relationships, to make up for it limited range of characters: Sid and Sondra spar in a friendly way, and Peter and Sondra have a believable attraction even though it's artificial and tainted (she is, after all, going to bed with a suspected homicidal maniac).<br /><br />What palls a bit is Allen's again drooling over English wealth and class, things his Brooklyn background seems to have left him, despite all his celebrity, with a irresistible hankering for. Jackman is an impressive fellow, glamorous and dashing. His parents were English. But could this athletic musical comedy star raised in Australia (\"X-Man's\" Wolverine) really pass as an aristocrat? Only in the movies, perhaps (here and in \"Kate and Leopold\").<br /><br />This isn't as strong a film as \"Match Point,\" but to say it's a loser as some viewers have is quite wrong. It has no more depth than a half-hour radio drama or a TV show, but Woody's jokes are far funnier and more original than you'll get in any such media affair, and sometimes they show a return to the old wit and cleverness. It doesn't matter if a movie is silly or slapdash when it's diverting summer entertainment. On a hot day you don't want a heavy meal. The whole thing deliciously evokes a time when movie comedies were really light escapist entertainment, without crude jokes or bombastic effects; without Vince Vaughan or Owen Wilson. Critics are eager to tell you this is a return to the Allen decline that preceded \"Match Point.\" Don't believe them. He doesn't try too hard. Why should he? He may be 70, but verbally, he's still light on his feet. And his body moves pretty fast too.\n",
      "1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first step in processing the reviews is to make sure that any html tags that appear should be removed. In addition I wish to tokenize our input, that way words such as *entertained* and *entertaining* are considered the same with regard to sentiment analysis."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import nltk\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.stem.porter import *\r\n",
    "\r\n",
    "import re\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "\r\n",
    "def review_to_words(review):\r\n",
    "    nltk.download(\"stopwords\", quiet=True)\r\n",
    "    stemmer = PorterStemmer()\r\n",
    "    \r\n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # remove HTML tags\r\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # convert to lower case\r\n",
    "    words = text.split() # split string into words\r\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # remove stopwords\r\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\r\n",
    "    \r\n",
    "    return words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `review_to_words` method defined above uses `BeautifulSoup` to remove any html tags that appear and uses the `nltk` package to tokenize the reviews. As a check to ensure we know how everything is working, I'll try applying `review_to_words` to one of the reviews in the training set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# apply review_to_words to a review \r\n",
    "review_to_words(train_X[100])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['scoop',\n",
       " 'also',\n",
       " 'name',\n",
       " 'late',\n",
       " 'thirti',\n",
       " 'evelyn',\n",
       " 'waugh',\n",
       " 'novel',\n",
       " 'woodi',\n",
       " 'allen',\n",
       " 'new',\n",
       " 'movi',\n",
       " 'though',\n",
       " 'set',\n",
       " 'today',\n",
       " 'nostalg',\n",
       " 'charm',\n",
       " 'simplic',\n",
       " 'depth',\n",
       " 'character',\n",
       " 'intens',\n",
       " 'perform',\n",
       " 'suspens',\n",
       " 'shock',\n",
       " 'final',\n",
       " 'frisson',\n",
       " 'allen',\n",
       " 'penultim',\n",
       " 'effort',\n",
       " 'match',\n",
       " 'point',\n",
       " 'argu',\n",
       " 'mani',\n",
       " 'includ',\n",
       " 'review',\n",
       " 'strong',\n",
       " 'return',\n",
       " 'form',\n",
       " 'scoop',\n",
       " 'close',\n",
       " 'resembl',\n",
       " 'allen',\n",
       " 'last',\n",
       " 'outing',\n",
       " 'focu',\n",
       " 'english',\n",
       " 'aristocrat',\n",
       " 'posh',\n",
       " 'london',\n",
       " 'flat',\n",
       " 'murder',\n",
       " 'detect',\n",
       " 'time',\n",
       " 'woodi',\n",
       " 'leav',\n",
       " 'behind',\n",
       " 'arrivist',\n",
       " 'murder',\n",
       " 'mysteri',\n",
       " 'genr',\n",
       " 'return',\n",
       " 'comedi',\n",
       " 'back',\n",
       " 'screen',\n",
       " 'amiabl',\n",
       " 'vaudevillian',\n",
       " 'magician',\n",
       " 'call',\n",
       " 'sid',\n",
       " 'waterman',\n",
       " 'stage',\n",
       " 'monik',\n",
       " 'great',\n",
       " 'splendini',\n",
       " 'counter',\n",
       " 'snob',\n",
       " 'probe',\n",
       " 'use',\n",
       " 'hebrew',\n",
       " 'persuas',\n",
       " 'got',\n",
       " 'older',\n",
       " 'convert',\n",
       " 'narciss',\n",
       " 'follow',\n",
       " 'revel',\n",
       " 'midst',\n",
       " 'splendini',\n",
       " 'standard',\n",
       " 'demateri',\n",
       " 'act',\n",
       " 'scarlett',\n",
       " 'johansson',\n",
       " 'sondra',\n",
       " 'pranski',\n",
       " 'audienc',\n",
       " 'volunt',\n",
       " 'mismatch',\n",
       " 'pair',\n",
       " 'get',\n",
       " 'drawn',\n",
       " 'dead',\n",
       " 'ace',\n",
       " 'english',\n",
       " 'journalist',\n",
       " 'post',\n",
       " 'mortem',\n",
       " 'attempt',\n",
       " 'score',\n",
       " 'one',\n",
       " 'last',\n",
       " 'top',\n",
       " 'news',\n",
       " 'stori',\n",
       " 'edg',\n",
       " 'styx',\n",
       " 'joe',\n",
       " 'strombel',\n",
       " 'ian',\n",
       " 'mcshane',\n",
       " 'met',\n",
       " 'shade',\n",
       " 'one',\n",
       " 'lord',\n",
       " 'lyman',\n",
       " 'son',\n",
       " 'secretari',\n",
       " 'say',\n",
       " 'poison',\n",
       " 'told',\n",
       " 'charm',\n",
       " 'aristocrat',\n",
       " 'bounder',\n",
       " 'son',\n",
       " 'peter',\n",
       " 'lyman',\n",
       " 'hugh',\n",
       " 'jackman',\n",
       " 'tarot',\n",
       " 'card',\n",
       " 'murder',\n",
       " 'london',\n",
       " 'serial',\n",
       " 'killer',\n",
       " 'sondra',\n",
       " 'sid',\n",
       " 'immedi',\n",
       " 'becom',\n",
       " 'pair',\n",
       " 'amateur',\n",
       " 'sleuth',\n",
       " 'sid',\n",
       " 'deadpan',\n",
       " 'wit',\n",
       " 'sondra',\n",
       " 'bumptiou',\n",
       " 'beauti',\n",
       " 'cut',\n",
       " 'quick',\n",
       " 'swath',\n",
       " 'cream',\n",
       " 'london',\n",
       " 'aristocraci',\n",
       " 'woodi',\n",
       " 'paw',\n",
       " 'young',\n",
       " 'heroin',\n",
       " 'muse',\n",
       " 'match',\n",
       " 'point',\n",
       " 'johansson',\n",
       " 'past',\n",
       " 'time',\n",
       " 'moreov',\n",
       " 'scarlett',\n",
       " 'ambiti',\n",
       " 'sexpot',\n",
       " 'would',\n",
       " 'movi',\n",
       " 'star',\n",
       " 'morph',\n",
       " 'surprisingli',\n",
       " 'klutzi',\n",
       " 'bespectacl',\n",
       " 'still',\n",
       " 'pretti',\n",
       " 'co',\n",
       " 'sid',\n",
       " 'sondra',\n",
       " 'flirtat',\n",
       " 'great',\n",
       " 'relief',\n",
       " 'simpli',\n",
       " 'team',\n",
       " 'less',\n",
       " 'polit',\n",
       " 'carri',\n",
       " 'strombel',\n",
       " 'wish',\n",
       " 'befriend',\n",
       " 'lyman',\n",
       " 'watch',\n",
       " 'clue',\n",
       " 'guilt',\n",
       " 'minim',\n",
       " 'protest',\n",
       " 'sid',\n",
       " 'consent',\n",
       " 'appear',\n",
       " 'sondra',\n",
       " 'dad',\n",
       " 'sondra',\n",
       " 'captiv',\n",
       " 'peter',\n",
       " 'pretend',\n",
       " 'drown',\n",
       " 'club',\n",
       " 'pool',\n",
       " 'christen',\n",
       " 'jade',\n",
       " 'spenc',\n",
       " 'mr',\n",
       " 'spenc',\n",
       " 'e',\n",
       " 'woodi',\n",
       " 'keep',\n",
       " 'break',\n",
       " 'cover',\n",
       " 'card',\n",
       " 'trick',\n",
       " 'amus',\n",
       " 'dowag',\n",
       " 'beat',\n",
       " 'husband',\n",
       " 'poker',\n",
       " 'spew',\n",
       " 'non',\n",
       " 'stop',\n",
       " 'one',\n",
       " 'liner',\n",
       " 'maintain',\n",
       " 'appar',\n",
       " 'success',\n",
       " 'oil',\n",
       " 'preciou',\n",
       " 'metal',\n",
       " 'jade',\n",
       " 'told',\n",
       " 'say',\n",
       " 'told',\n",
       " 'without',\n",
       " 'spoil',\n",
       " 'stori',\n",
       " 'reveal',\n",
       " 'outcom',\n",
       " 'first',\n",
       " 'allen',\n",
       " 'decis',\n",
       " 'make',\n",
       " 'johansson',\n",
       " 'gauch',\n",
       " 'naiv',\n",
       " 'plainspoken',\n",
       " 'badli',\n",
       " 'dress',\n",
       " 'colleg',\n",
       " 'girl',\n",
       " 'seem',\n",
       " 'unkind',\n",
       " 'around',\n",
       " 'bad',\n",
       " 'decis',\n",
       " 'johansson',\n",
       " 'pluck',\n",
       " 'panach',\n",
       " 'actress',\n",
       " 'miracul',\n",
       " 'manag',\n",
       " 'carri',\n",
       " 'help',\n",
       " 'jackman',\n",
       " 'actor',\n",
       " 'know',\n",
       " 'make',\n",
       " 'actress',\n",
       " 'appear',\n",
       " 'desir',\n",
       " 'desir',\n",
       " 'film',\n",
       " 'actual',\n",
       " 'creat',\n",
       " 'sens',\n",
       " 'relationship',\n",
       " 'make',\n",
       " 'limit',\n",
       " 'rang',\n",
       " 'charact',\n",
       " 'sid',\n",
       " 'sondra',\n",
       " 'spar',\n",
       " 'friendli',\n",
       " 'way',\n",
       " 'peter',\n",
       " 'sondra',\n",
       " 'believ',\n",
       " 'attract',\n",
       " 'even',\n",
       " 'though',\n",
       " 'artifici',\n",
       " 'taint',\n",
       " 'go',\n",
       " 'bed',\n",
       " 'suspect',\n",
       " 'homicid',\n",
       " 'maniac',\n",
       " 'pall',\n",
       " 'bit',\n",
       " 'allen',\n",
       " 'drool',\n",
       " 'english',\n",
       " 'wealth',\n",
       " 'class',\n",
       " 'thing',\n",
       " 'brooklyn',\n",
       " 'background',\n",
       " 'seem',\n",
       " 'left',\n",
       " 'despit',\n",
       " 'celebr',\n",
       " 'irresist',\n",
       " 'hanker',\n",
       " 'jackman',\n",
       " 'impress',\n",
       " 'fellow',\n",
       " 'glamor',\n",
       " 'dash',\n",
       " 'parent',\n",
       " 'english',\n",
       " 'could',\n",
       " 'athlet',\n",
       " 'music',\n",
       " 'comedi',\n",
       " 'star',\n",
       " 'rais',\n",
       " 'australia',\n",
       " 'x',\n",
       " 'man',\n",
       " 'wolverin',\n",
       " 'realli',\n",
       " 'pass',\n",
       " 'aristocrat',\n",
       " 'movi',\n",
       " 'perhap',\n",
       " 'kate',\n",
       " 'leopold',\n",
       " 'strong',\n",
       " 'film',\n",
       " 'match',\n",
       " 'point',\n",
       " 'say',\n",
       " 'loser',\n",
       " 'viewer',\n",
       " 'quit',\n",
       " 'wrong',\n",
       " 'depth',\n",
       " 'half',\n",
       " 'hour',\n",
       " 'radio',\n",
       " 'drama',\n",
       " 'tv',\n",
       " 'show',\n",
       " 'woodi',\n",
       " 'joke',\n",
       " 'far',\n",
       " 'funnier',\n",
       " 'origin',\n",
       " 'get',\n",
       " 'media',\n",
       " 'affair',\n",
       " 'sometim',\n",
       " 'show',\n",
       " 'return',\n",
       " 'old',\n",
       " 'wit',\n",
       " 'clever',\n",
       " 'matter',\n",
       " 'movi',\n",
       " 'silli',\n",
       " 'slapdash',\n",
       " 'divert',\n",
       " 'summer',\n",
       " 'entertain',\n",
       " 'hot',\n",
       " 'day',\n",
       " 'want',\n",
       " 'heavi',\n",
       " 'meal',\n",
       " 'whole',\n",
       " 'thing',\n",
       " 'delici',\n",
       " 'evok',\n",
       " 'time',\n",
       " 'movi',\n",
       " 'comedi',\n",
       " 'realli',\n",
       " 'light',\n",
       " 'escapist',\n",
       " 'entertain',\n",
       " 'without',\n",
       " 'crude',\n",
       " 'joke',\n",
       " 'bombast',\n",
       " 'effect',\n",
       " 'without',\n",
       " 'vinc',\n",
       " 'vaughan',\n",
       " 'owen',\n",
       " 'wilson',\n",
       " 'critic',\n",
       " 'eager',\n",
       " 'tell',\n",
       " 'return',\n",
       " 'allen',\n",
       " 'declin',\n",
       " 'preced',\n",
       " 'match',\n",
       " 'point',\n",
       " 'believ',\n",
       " 'tri',\n",
       " 'hard',\n",
       " 'may',\n",
       " '70',\n",
       " 'verbal',\n",
       " 'still',\n",
       " 'light',\n",
       " 'feet',\n",
       " 'bodi',\n",
       " 'move',\n",
       " 'pretti',\n",
       " 'fast']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above `review_to_words` method removes html formatting and allows us to tokenize the words found in a review, for example, converting *entertained* and *entertaining* into *entertain* so that they are treated as though they are the same word, it also removes stop words."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The method below applies the `review_to_words` method to each of the reviews in the training and testing datasets. In addition it caches the results. This is because performing this processing step can take a long time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import pickle\r\n",
    "\r\n",
    "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\r\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\r\n",
    "\r\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\r\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\r\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\r\n",
    "\r\n",
    "    # If cache_file is not None, try to read from it first\r\n",
    "    cache_data = None\r\n",
    "    if cache_file is not None:\r\n",
    "        try:\r\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\r\n",
    "                cache_data = pickle.load(f)\r\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\r\n",
    "        except:\r\n",
    "            pass  # unable to read from cache, but that's okay\r\n",
    "    \r\n",
    "    # If cache is missing, then do the heavy lifting\r\n",
    "    if cache_data is None:\r\n",
    "        # Preprocess training and test data to obtain words for each review\r\n",
    "        words_train = [review_to_words(review) for review in data_train]\r\n",
    "        words_test = [review_to_words(review) for review in data_test]\r\n",
    "        \r\n",
    "        # Write to cache file for future runs\r\n",
    "        if cache_file is not None:\r\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\r\n",
    "                              labels_train=labels_train, labels_test=labels_test)\r\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\r\n",
    "                pickle.dump(cache_data, f)\r\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\r\n",
    "    else:\r\n",
    "        # Unpack data loaded from cache file\r\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\r\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\r\n",
    "    \r\n",
    "    return words_train, words_test, labels_train, labels_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Preprocess data\r\n",
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transforming the data\r\n",
    "\r\n",
    "For the model I'm going to construct in this notebook I will construct a feature representation. To start, I will represent each word as an integer. Of course, some of the words that appear in the reviews occur very infrequently and so likely don't contain much information for the purposes of sentiment analysis. The way I will deal with this problem is that I will fix the size of our working vocabulary and I will only include the words that appear most frequently. I will then combine all of the infrequent words into a single category and, in our case, I will label it as `1`.\r\n",
    "\r\n",
    "Since I will be using a recurrent neural network, it will be convenient if the length of each review is the same. To do this, I will fix the size for our reviews and then pad short reviews with the category 'no word' (which I will label `0`) and truncate long reviews."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a word dictionary\r\n",
    "\r\n",
    "To begin with, I need to construct a way to map words that appear in the reviews to integers. Here I fix the size of our vocabulary (including the 'no word' and 'infrequent' categories) to be `5000`.\r\n",
    "\r\n",
    "Note that even though the vocab_size is set to `5000`, I only want to construct a mapping for the most frequently appearing `4998` words. This is because I want to reserve the special labels `0` for 'no word' and `1` for 'infrequent word'."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "\r\n",
    "def build_dict(data, vocab_size = 5000):\r\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\r\n",
    "    \r\n",
    "    # determine how often each word appears in `data`. Note that `data` is a list of sentences and that a sentence is a list of words.\r\n",
    "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\r\n",
    "    for review in data:\r\n",
    "        for word in review:\r\n",
    "            if word not in word_count:\r\n",
    "                word_count[word] = 1\r\n",
    "            else:\r\n",
    "                word_count[word] += 1\r\n",
    "    # sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and \r\n",
    "    # sorted_words[-1] is the least frequently appearing word.\r\n",
    "    \r\n",
    "    sorted_words = sorted(word_count,key=word_count.get,reverse=True)\r\n",
    "    \r\n",
    "    word_dict = {} # This is what I'm are building, a dictionary that translates words into integers\r\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\r\n",
    "        word_dict[word] = idx + 2                              # 'infrequent' labels\r\n",
    "        \r\n",
    "    return word_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "word_dict = build_dict(train_X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# determine the five most frequently appearing words in the training set.\r\n",
    "list(word_dict.items())[:5]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('movi', 2), ('film', 3), ('one', 4), ('like', 5), ('time', 6)]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save `word_dict`\r\n",
    "\r\n",
    "Later on when I construct an endpoint which processes a submitted review, I will need to make use of the `word_dict` which I have created. As such, I will save it to a file now for future use."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "data_dir = '../data/pytorch' # the folder I will use for storing data\r\n",
    "if not os.path.exists(data_dir): # make sure that the folder exists\r\n",
    "    os.makedirs(data_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\r\n",
    "    pickle.dump(word_dict, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transform the reviews\r\n",
    "\r\n",
    "Now that I have the word dictionary which allows us to transform the words appearing in the reviews into integers, it's time to make use of it and convert our reviews to their integer sequence representation, making sure to pad or truncate to a fixed length, which in our case is `500`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def convert_and_pad(word_dict, sentence, pad=500):\r\n",
    "    NOWORD = 0 # I will use 0 to represent the 'no word' category\r\n",
    "    INFREQ = 1 # and use 1 to represent the infrequent words, i.e., words not appearing in word_dict\r\n",
    "    \r\n",
    "    working_sentence = [NOWORD] * pad\r\n",
    "    \r\n",
    "    for word_index, word in enumerate(sentence[:pad]):\r\n",
    "        if word in word_dict:\r\n",
    "            working_sentence[word_index] = word_dict[word]\r\n",
    "        else:\r\n",
    "            working_sentence[word_index] = INFREQ\r\n",
    "            \r\n",
    "    return working_sentence, min(len(sentence), pad)\r\n",
    "\r\n",
    "def convert_and_pad_data(word_dict, data, pad=500):\r\n",
    "    result = []\r\n",
    "    lengths = []\r\n",
    "    \r\n",
    "    for sentence in data:\r\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\r\n",
    "        result.append(converted)\r\n",
    "        lengths.append(leng)\r\n",
    "        \r\n",
    "    return np.array(result), np.array(lengths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\r\n",
    "test_X, test_X_len = convert_and_pad_data(word_dict, test_X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a quick check to make sure that things are working as intended, I'll check to see what one of the reviews in the training set looks like after having been processeed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# examine one of the processed reviews to make sure everything is working as intended.\r\n",
    "print(train_X[0])\r\n",
    "print(\"Review length:\",len(train_X[0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[  45  575 1212    2    5  419  629  753 1113   75 3665  514   85 1096\n",
      "    2  187  102    2  584  634  164   97 1412  774 1219    1 4095    1\n",
      "    5  343 1805 1436 1425    1    1  262 1263  645  229  123 1444    1\n",
      "  194  132  535    1    1 1707 3961  378    1   57  758  136  943 4095\n",
      "   34  758 1242  303 1623 1345   33   75 1910  212 1444   33  740  324\n",
      "   64 4095  163 1444 2580  318  194  811  300    4   26    2   24   35\n",
      "    2 1841 1408 2180 3606  137   10    1   17 1012    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "Review length: 500\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the cells above I use the `preprocess_data` and `convert_and_pad_data` methods to process both the training and testing set, this might result in a a problem beacuse the most frequent words (the vocabulary) can be different in the testing set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Upload the data to S3\r\n",
    "\r\n",
    "As in the XGBoost notebook, I will need to upload the training dataset to S3 in order for our training code to access it. For now I will save it locally and I will upload to S3 later on.\r\n",
    "\r\n",
    "### Saving the processed training dataset locally\r\n",
    "\r\n",
    "It is important to note the format of the data that I'm saving as I will need to know it when I write the training code. In our case, each row of the dataset has the form `label`, `length`, `review[500]` where `review[500]` is a sequence of `500` integers representing the words in the review."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import pandas as pd\r\n",
    "    \r\n",
    "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) \\\r\n",
    "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Uploading the training data\r\n",
    "\r\n",
    "\r\n",
    "Next, I need to upload the training data to the SageMaker default S3 bucket so that I can provide access to it while training our model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import sagemaker\r\n",
    "\r\n",
    "sagemaker_session = sagemaker.Session()\r\n",
    "\r\n",
    "bucket = sagemaker_session.default_bucket()\r\n",
    "prefix = 'sagemaker/sentiment_rnn'\r\n",
    "\r\n",
    "role = sagemaker.get_execution_role()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**NOTE:** The cell above uploads the entire contents of our data directory. This includes the `word_dict.pkl` file. This is fortunate as I will need this later on when I create an endpoint that accepts an arbitrary review. For now, I will just take note of the fact that it resides in the data directory (and so also in the S3 training bucket) and that I will need to make sure it gets saved in the model directory."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Building and Training the PyTorch Model\r\n",
    "\r\n",
    "A model comprises three objects:\r\n",
    "\r\n",
    " - Model Artifacts,\r\n",
    " - Training Code, and\r\n",
    " - Inference Code,\r\n",
    " \r\n",
    "each of which interact with one another. I will be using containers provided by Amazon with the added benefit of being able to include my own custom code.\r\n",
    "\r\n",
    "I will start by implementing my own neural network in PyTorch along with a training script. the model object is in the `model.py` file, inside of the `train` folder. we can see the implementation by running the cell below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "!pygmentize train/model.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mLSTMClassifier\u001b[39;49;00m(nn.Module):\r\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m    This is the simple RNN model we will be using to perform Sentiment Analysis.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, embedding_dim, hidden_dim, vocab_size):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Initialize the model by settingg up the various layers.\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(LSTMClassifier, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=\u001b[34m0\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.lstm = nn.LSTM(embedding_dim, hidden_dim)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.dense = nn.Linear(in_features=hidden_dim, out_features=\u001b[34m1\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.sig = nn.Sigmoid()\r\n",
      "        \r\n",
      "        \u001b[36mself\u001b[39;49;00m.word_dict = \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Perform a forward pass of our model on some input.\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        x = x.t()\r\n",
      "        lengths = x[\u001b[34m0\u001b[39;49;00m,:]\r\n",
      "        reviews = x[\u001b[34m1\u001b[39;49;00m:,:]\r\n",
      "        embeds = \u001b[36mself\u001b[39;49;00m.embedding(reviews)\r\n",
      "        lstm_out, _ = \u001b[36mself\u001b[39;49;00m.lstm(embeds)\r\n",
      "        out = \u001b[36mself\u001b[39;49;00m.dense(lstm_out)\r\n",
      "        out = out[lengths - \u001b[34m1\u001b[39;49;00m, \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(lengths))]\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.sig(out.squeeze())\r\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The important takeaway from the implementation is that there are three parameters that I may wish to tweak to improve the performance of the model. These are the embedding dimension, the hidden dimension and the size of the vocabulary. I will likely want to make these parameters configurable in the training script so that if I wish to modify them I do not need to modify the script itself. I will see how to do this later on. To start I will write some of the training code in the notebook so that I can more easily diagnose any issues that arise.\r\n",
    "\r\n",
    "First I will load a small portion of the training data set to use as a sample. It would be very time consuming to try and train the model completely in the notebook as I do not have access to a gpu and the compute instance that I'm using is not particularly powerful. However, I can work on a small bit of the data to get a feel for how our training script is behaving."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import torch\r\n",
    "import torch.utils.data\r\n",
    "\r\n",
    "# Read in only the first 250 rows\r\n",
    "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)\r\n",
    "\r\n",
    "# Turn the input pandas dataframe into tensors\r\n",
    "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\r\n",
    "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\r\n",
    "\r\n",
    "# Build the dataset\r\n",
    "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\r\n",
    "# Build the dataloader\r\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Writing the training method"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def train(model, train_loader, epochs, optimizer, loss_fn, device):\r\n",
    "    for epoch in range(1, epochs + 1):\r\n",
    "        model.train()\r\n",
    "        total_loss = 0\r\n",
    "        for batch in train_loader:         \r\n",
    "            batch_X, batch_y = batch\r\n",
    "            \r\n",
    "            batch_X = batch_X.to(device)\r\n",
    "            batch_y = batch_y.to(device)\r\n",
    "            \r\n",
    "            # complete this train method to train the model provided.\r\n",
    "            optimizer.zero_grad()\r\n",
    "            output = model(batch_X)\r\n",
    "            loss = loss_fn(output,batch_y)\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            \r\n",
    "            total_loss += loss.data.item()\r\n",
    "        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "knowing that I have the training method above, I will test that it is working by writing a bit of code in the notebook that executes our training method on the small sample training set that I loaded earlier. The reason for doing this in the notebook is so that I have an opportunity to fix any errors that arise early when they are easier to diagnose."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import torch.optim as optim\r\n",
    "from train.model import LSTMClassifier\r\n",
    "\r\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "model = LSTMClassifier(32, 100, 5000).to(device)\r\n",
    "optimizer = optim.Adam(model.parameters())\r\n",
    "loss_fn = torch.nn.BCELoss()\r\n",
    "\r\n",
    "train(model, train_sample_dl, 5, optimizer, loss_fn, device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1, BCELoss: 0.6935802578926087\n",
      "Epoch: 2, BCELoss: 0.6845847368240356\n",
      "Epoch: 3, BCELoss: 0.6771884322166443\n",
      "Epoch: 4, BCELoss: 0.668962550163269\n",
      "Epoch: 5, BCELoss: 0.6587221145629882\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to construct a PyTorch model using SageMaker I must provide SageMaker with a training script. I could optionally include a directory which will be copied to the container and from which the training code will be run. When the training container is executed it will check the uploaded directory (if there is one) for a `requirements.txt` file and install any required Python libraries, after which the training script will be run."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the model\r\n",
    "\r\n",
    "When a PyTorch model is constructed in SageMaker, an entry point must be specified. This is the Python file which will be executed when the model is trained. Inside of the `train` directory is a file called `train.py` which contains the necessary code to train the model.\r\n",
    "\r\n",
    "\r\n",
    "The way that SageMaker passes hyperparameters to the training script is by way of arguments. These arguments can then be parsed and used in the training script."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "from sagemaker.pytorch import PyTorch\r\n",
    "\r\n",
    "estimator = PyTorch(entry_point=\"train.py\",\r\n",
    "                    source_dir=\"train\",\r\n",
    "                    role=role,\r\n",
    "                    framework_version='0.4.0',\r\n",
    "                    train_instance_count=1,\r\n",
    "                    train_instance_type='ml.p2.xlarge',\r\n",
    "                    hyperparameters={\r\n",
    "                        'epochs': 10,\r\n",
    "                        'hidden_dim': 200,\r\n",
    "                    })"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "estimator.fit({'training': input_data})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-09-03 16:11:39 Starting - Starting the training job...\n",
      "2021-09-03 16:11:41 Starting - Launching requested ML instances......\n",
      "2021-09-03 16:12:45 Starting - Preparing the instances for training......\n",
      "2021-09-03 16:14:05 Downloading - Downloading input data...\n",
      "2021-09-03 16:14:39 Training - Downloading the training image...\n",
      "2021-09-03 16:15:10 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-09-03 16:15:11,531 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-09-03 16:15:11,556 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-09-03 16:15:12,976 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-09-03 16:15:13,228 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2021-09-03 16:15:13,229 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2021-09-03 16:15:13,229 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2021-09-03 16:15:13,229 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/b5/36/88723426b4ff576809fec7d73594fe17a35c27f8d01f93637637a29ae25b/numpy-1.18.5-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\u001b[0m\n",
      "\u001b[34mCollecting nltk (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\u001b[0m\n",
      "\u001b[34mCollecting beautifulsoup4 (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\u001b[0m\n",
      "\u001b[34mCollecting html5lib (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/dd/a834df6482147d48e225a49515aabc28974ad5a4ca3215c18a882565b028/html5lib-1.1-py2.py3-none-any.whl (112kB)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/70/94/784178ca5dd892a98f113cdd923372024dc04b8d40abe77ca76b5fb90ca6/pytz-2021.1-py2.py3-none-any.whl (510kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.5/dist-packages (from nltk->-r requirements.txt (line 3)) (7.0)\u001b[0m\n",
      "\u001b[34mCollecting joblib (from nltk->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\u001b[0m\n",
      "\u001b[34mCollecting regex (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/15/bd/88d793c2e39b1e91c070bf4d1317db599b1c22efbf6bd194bb568064af21/regex-2021.8.28.tar.gz (694kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/9c/05/cf212f57daa0eb6106fa668a04d74d932e9881fd4a22f322ea1dadb5aba0/tqdm-4.62.2-py2.py3-none-any.whl (76kB)\u001b[0m\n",
      "\u001b[34mCollecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/02/fb/1c65691a9aeb7bd6ac2aa505b84cb8b49ac29c976411c6ab3659425e045f/soupsieve-2.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.5/dist-packages (from html5lib->-r requirements.txt (line 5)) (1.11.0)\u001b[0m\n",
      "\u001b[34mCollecting webencodings (from html5lib->-r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train, regex\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ez6v1g55/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n",
      "  Running setup.py bdist_wheel for regex: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for regex: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/8d/4b/8a/08ac98dc33c5ca3e55527271fc5a892d3852cea155dd4260de\u001b[0m\n",
      "\u001b[34mSuccessfully built train regex\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, numpy, pandas, joblib, regex, tqdm, nltk, soupsieve, beautifulsoup4, webencodings, html5lib, train\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed beautifulsoup4-4.9.3 html5lib-1.1 joblib-0.14.1 nltk-3.6.2 numpy-1.18.5 pandas-0.24.2 pytz-2021.1 regex-2021.8.28 soupsieve-2.1 tqdm-4.62.2 train-1.0.0 webencodings-0.5.1\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.3.4 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2021-09-03 16:15:35,041 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-521600510407/sagemaker-pytorch-2021-09-03-16-11-39-317/source/sourcedir.tar.gz\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\"\n",
      "        }\n",
      "    },\n",
      "    \"num_gpus\": 1,\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"resource_config\": {\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ]\n",
      "    },\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"user_entry_point\": \"train.py\",\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"job_name\": \"sagemaker-pytorch-2021-09-03-16-11-39-317\",\n",
      "    \"hyperparameters\": {\n",
      "        \"hidden_dim\": 200,\n",
      "        \"epochs\": 10\n",
      "    },\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"log_level\": 20,\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    }\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-521600510407/sagemaker-pytorch-2021-09-03-16-11-39-317/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_dim\":200}\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_dim\":200},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2021-09-03-16-11-39-317\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-521600510407/sagemaker-pytorch-2021-09-03-16-11-39-317/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_dim\",\"200\"]\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=200\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 10 --hidden_dim 200\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cuda.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mModel loaded with embedding_dim 32, hidden_dim 200, vocab_size 5000.\u001b[0m\n",
      "\u001b[34mEpoch: 1, BCELoss: 0.6707386435294638\u001b[0m\n",
      "\u001b[34mEpoch: 2, BCELoss: 0.5835666133432972\u001b[0m\n",
      "\u001b[34mEpoch: 3, BCELoss: 0.49056457986637036\u001b[0m\n",
      "\u001b[34mEpoch: 4, BCELoss: 0.412860813189526\u001b[0m\n",
      "\u001b[34mEpoch: 5, BCELoss: 0.404849112033844\u001b[0m\n",
      "\u001b[34mEpoch: 6, BCELoss: 0.3449799229904097\u001b[0m\n",
      "\u001b[34mEpoch: 7, BCELoss: 0.3115262729781015\u001b[0m\n",
      "\u001b[34mEpoch: 8, BCELoss: 0.30520691251268195\u001b[0m\n",
      "\u001b[34mEpoch: 9, BCELoss: 0.29014194954414757\u001b[0m\n",
      "\n",
      "2021-09-03 16:18:42 Uploading - Uploading generated training model\n",
      "2021-09-03 16:18:42 Completed - Training job completed\n",
      "\u001b[34mEpoch: 10, BCELoss: 0.2732922237138359\u001b[0m\n",
      "\u001b[34m2021-09-03 16:18:33,052 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 277\n",
      "Billable seconds: 277\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Testing the model\r\n",
    "\r\n",
    "As mentioned at the top of this notebook, I will be testing this model by first deploying it and then sending the testing data to the deployed endpoint. I will do this so that i can make sure that the deployed model is working correctly.\r\n",
    "\r\n",
    "## Step 6: Deploy the model for testing\r\n",
    "\r\n",
    "Now that I have trained the model, I would like to test it to see how it performs. Currently the model takes input of the form `review_length, review[500]` where `review[500]` is a sequence of `500` integers which describe the words present in the review, encoded using `word_dict`. Fortunately, SageMaker provides built-in inference code for models with simple inputs such as this.\r\n",
    "\r\n",
    "There is one thing that I need to provide, however, and that is a function which loads the saved model. This function will be called `model_fn()` and takes as its only parameter a path to the directory where the model artifacts are stored. This function is also be present in the python file which I specified as the entry point.\r\n",
    "\r\n",
    "Since I don't need to change anything in the code that was uploaded during training, I can simply deploy the current model as-is.\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# deploy the trained model\r\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------!"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 7 - Using the model for testing\r\n",
    "\r\n",
    "Once deployed, I can read in the test data and send it off to the deployed model to get some results. Once I collect all of the results, I can determine how accurate the model is."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# split the data into chunks and send each chunk seperately, accumulating the results.\r\n",
    "\r\n",
    "def predict(data, rows=512):\r\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\r\n",
    "    predictions = np.array([])\r\n",
    "    for array in split_array:\r\n",
    "        predictions = np.append(predictions, predictor.predict(array))\r\n",
    "    \r\n",
    "    return predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "predictions = predict(test_X.values)\r\n",
    "predictions = [round(num) for num in predictions]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "from sklearn.metrics import accuracy_score\r\n",
    "accuracy_score(test_y, predictions)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.85276"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### More testing\r\n",
    "\r\n",
    "I now have a trained model which has been deployed and which I can send processed reviews to and which returns the predicted sentiment. However, ultimately I would like to be able to send the model an unprocessed review. That is, I would like to send the review itself as a string. For example, I wish to send the following review to the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "test_review = 'The simplest pleasures in life are the best, and this film is one of them. Combining a rather basic storyline of love and adventure this movie transcends the usual weekend fair with wit and unmitigated charm.'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "In order process the review I will need to repeat these two steps:\r\n",
    " - Removed any html tags and stemmed the input\r\n",
    " - Encoded the review as a sequence of integers using `word_dict`\r\n",
    " \r\n",
    "Using the `review_to_words` and `convert_and_pad` methods from section one, I will convert `test_review` into a numpy array `test_data` suitable to send to our model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# convert test_review into a form usable by the model and save the results in test_data\r\n",
    "test_data, len_test  = convert_and_pad(word_dict, review_to_words(test_review))\r\n",
    "test_data = np.array([np.array([len_test] + test_data)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that I have processed the review, I can send the resulting array to the model to predict the sentiment of the review."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "predictor.predict(test_data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(0.8228836, dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the return value of the model is close to `1`, I can be certain that the review I submitted is positive."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deleting the endpoint\r\n",
    "\r\n",
    "Once I've deployed an endpoint it continues to run until I tell it to shut down. Since I'm done using the endpoint for now, I can delete it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "estimator.delete_endpoint()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "estimator.delete_endpoint() will be deprecated in SageMaker Python SDK v2. Please use the delete_endpoint() function on your predictor instead.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 6 (again) - Deploy the model for the web app\r\n",
    "\r\n",
    "Now that I know that the model is working, it's time to create some custom inference code so that I can send the model a review which has not been processed and have it determine the sentiment of the review.\r\n",
    "\r\n",
    "As we saw above, by default the estimator which I created, when deployed, will use the entry script and directory which I provided when creating the model. However, since I now wish to accept a string as input and the model expects a processed review, I need to write some custom inference code.\r\n",
    "\r\n",
    "I will store the code that I wrote in the `serve` directory. Provided in this directory is the `model.py` file that I used to construct the model, a `utils.py` file which contains the `review_to_words` and `convert_and_pad` pre-processing functions which I used during the initial data processing, and `predict.py`, the file which will contain our custom inference code. Note also that `requirements.txt` is present which will tell SageMaker what Python libraries are required by our custom inference code.\r\n",
    "\r\n",
    "When deploying a PyTorch model in SageMaker, I'm expected to provide four functions which the SageMaker inference container will use.\r\n",
    " - `model_fn`: This function is the same function that I used in the training script and it tells SageMaker how to load our model.\r\n",
    " - `input_fn`: This function receives the raw serialized input that has been sent to the model's endpoint and its job is to de-serialize and make the input available for the inference code.\r\n",
    " - `output_fn`: This function takes the output of the inference code and its job is to serialize this output and return it to the caller of the model's endpoint.\r\n",
    " - `predict_fn`: The heart of the inference script, this is where the actual prediction is done.\r\n",
    "\r\n",
    "For the simple website that I'm constructing during this project, the `input_fn` and `output_fn` methods are relatively straightforward. We only require being able to accept a string as input and we expect to return a single value as output. You might imagine though that in a more complex application the input or output may be image data or some other binary data which would require some effort to serialize.\r\n",
    "\r\n",
    "### Writing inference code\r\n",
    "\r\n",
    "Before writing the custom inference code, I will begin by taking a look at the code created."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "!pygmentize serve/predict.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LSTMClassifier\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m review_to_words, convert_and_pad\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n",
      "    \u001b[33m\"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\r\n",
      "    model_info = {}\r\n",
      "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model_info = torch.load(f)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\r\n",
      "\r\n",
      "    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    model = LSTMClassifier(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33membedding_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mvocab_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    \u001b[37m# Load the store model parameters.\u001b[39;49;00m\r\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.load_state_dict(torch.load(f))\r\n",
      "\r\n",
      "    \u001b[37m# Load the saved word_dict.\u001b[39;49;00m\r\n",
      "    word_dict_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mword_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(word_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.word_dict = pickle.load(f)\r\n",
      "\r\n",
      "    model.to(device).eval()\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mDeserializing the input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mif\u001b[39;49;00m content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mtext/plain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        data = serialized_input_data.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m data\r\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in content_type: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + content_type)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSerializing the generated output.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(prediction_output)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mInferring sentiment of input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[34mif\u001b[39;49;00m model.word_dict \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel has not been loaded properly, no word_dict.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m# TODO: Process input_data so that it is ready to be sent to our model.\u001b[39;49;00m\r\n",
      "    \u001b[37m#       You should produce two variables:\u001b[39;49;00m\r\n",
      "    \u001b[37m#         data_X   - A sequence of length 500 which represents the converted review\u001b[39;49;00m\r\n",
      "    \u001b[37m#         data_len - The length of the review\u001b[39;49;00m\r\n",
      "\r\n",
      "    data_X, data_len = convert_and_pad(model.word_dict, review_to_words(input_data))\r\n",
      "\r\n",
      "    \u001b[37m# Using data_X and data_len we construct an appropriate input tensor. Remember\u001b[39;49;00m\r\n",
      "    \u001b[37m# that our model expects input data of the form 'len, review[500]'.\u001b[39;49;00m\r\n",
      "    data_pack = np.hstack((data_len, data_X))\r\n",
      "    data_pack = data_pack.reshape(\u001b[34m1\u001b[39;49;00m, -\u001b[34m1\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    data = torch.from_numpy(data_pack)\r\n",
      "    data = data.to(device)\r\n",
      "\r\n",
      "    \u001b[37m# Make sure to put the model into evaluation mode\u001b[39;49;00m\r\n",
      "    model.eval()\r\n",
      "\r\n",
      "    \u001b[37m# TODO: Compute the result of applying the model to the input data. The variable `result` should\u001b[39;49;00m\r\n",
      "    \u001b[37m#       be a numpy array which contains a single integer which is either 1 or 0\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\r\n",
      "        \r\n",
      "        output = model(data)\r\n",
      "        \r\n",
      "    result = np.round(output.numpy()).astype(\u001b[36mint\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m result\r\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deploying the model\r\n",
    "\r\n",
    "Now the custom inference code has been added in the `serve/predict.py` file, I will create and deploy the model. To begin with, I need to construct a new PyTorch Model object which points to the model artifacts created during training and also points to the inference code that I wish to use. Then I can call the deploy method to launch the deployment container.\r\n",
    "\r\n",
    "**NOTE**: The default behaviour for a deployed PyTorch model is to assume that any input passed to the predictor is a `numpy` array. In this case I want to send a string so I need to construct a simple wrapper around the `RealTimePredictor` class to accomodate simple strings. In a more complicated situation I may want to provide a serialization object, for example if I wanted to sent image data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "from sagemaker.predictor import RealTimePredictor\r\n",
    "from sagemaker.pytorch import PyTorchModel\r\n",
    "\r\n",
    "class StringPredictor(RealTimePredictor):\r\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\r\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\r\n",
    "\r\n",
    "model = PyTorchModel(model_data=estimator.model_data,\r\n",
    "                     role = role,\r\n",
    "                     framework_version='0.4.0',\r\n",
    "                     entry_point='predict.py',\r\n",
    "                     source_dir='serve',\r\n",
    "                     predictor_cls=StringPredictor)\r\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------!"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing the model\r\n",
    "\r\n",
    "Now that I have deployed the model with the custom inference code, I should test to see if everything is working. Here, I test our model by loading the first `250` positive and negative reviews and send them to the endpoint, then collect the results. The reason for only sending some of the data is that the amount of time it takes for the model to process the input and then perform inference is quite long and so testing the entire data set would be prohibitive."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "import glob\r\n",
    "\r\n",
    "def test_reviews(data_dir='../data/aclImdb', stop=250):\r\n",
    "    \r\n",
    "    results = []\r\n",
    "    ground = []\r\n",
    "    \r\n",
    "    # I make sure to test both positive and negative reviews    \r\n",
    "    for sentiment in ['pos', 'neg']:\r\n",
    "        \r\n",
    "        path = os.path.join(data_dir, 'test', sentiment, '*.txt')\r\n",
    "        files = glob.glob(path)\r\n",
    "        \r\n",
    "        files_read = 0\r\n",
    "        \r\n",
    "        print('Starting ', sentiment, ' files')\r\n",
    "        \r\n",
    "        # Iterate through the files and send them to the predictor\r\n",
    "        for f in files:\r\n",
    "            with open(f) as review:\r\n",
    "                # First, I store the ground truth (was the review positive or negative)\r\n",
    "                if sentiment == 'pos':\r\n",
    "                    ground.append(1)\r\n",
    "                else:\r\n",
    "                    ground.append(0)\r\n",
    "                # Read in the review and convert to 'utf-8' for transmission via HTTP\r\n",
    "                review_input = review.read().encode('utf-8')\r\n",
    "                # Send the review to the predictor and store the results\r\n",
    "                results.append(float(predictor.predict(review_input)))\r\n",
    "                \r\n",
    "            # Sending reviews to our endpoint one at a time takes a while so I\r\n",
    "            # only send a small number of reviews\r\n",
    "            files_read += 1\r\n",
    "            if files_read == stop:\r\n",
    "                break\r\n",
    "            \r\n",
    "    return ground, results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "ground, results = test_reviews()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting  pos  files\n",
      "Starting  neg  files\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "from sklearn.metrics import accuracy_score\r\n",
    "accuracy_score(ground, results)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.846"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As an additional test, I will try sending the `test_review` that we looked at earlier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "predictor.predict(test_review)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "b'1'"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that I know our endpoint is working as expected, I can set up the web page that will interact with it. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 7 (again): Using the model for the web app\r\n",
    "\r\n",
    "This entire section and the next contain documentation for the complete deployment process mostly done using the AWS console.\r\n",
    "\r\n",
    "So far I have been accessing the model endpoint by constructing a predictor object which uses the endpoint and then just using the predictor object to perform inference. What if I wanted to create a web app which accesses our model? The way things are set up currently makes that not possible since in order to access a SageMaker endpoint the app would first have to authenticate with AWS using an IAM role which included access to SageMaker endpoints. However, there is an easier way! I just need to use some additional AWS services.\r\n",
    "\r\n",
    "<img src=\"Web App Diagram.svg\">\r\n",
    "\r\n",
    "The diagram above gives an overview of how the various services will work together. On the far right is the model which I trained above and which is deployed using SageMaker. On the far left is the web app that collects a user's movie review, sends it off and expects a positive or negative sentiment in return.\r\n",
    "\r\n",
    "In the middle is where some of the magic happens. I will construct a Lambda function, which can be thought of as a straightforward Python function that can be executed whenever a specified event occurs. i will give this function permission to send and recieve data from a SageMaker endpoint.\r\n",
    "\r\n",
    "Lastly, the method I will use to execute the Lambda function is a new endpoint that I will create using API Gateway. This endpoint will be a url that listens for data to be sent to it. Once it gets some data it will pass that data on to the Lambda function and then return whatever the Lambda function returns. Essentially it will act as an interface that lets the web app communicate with the Lambda function.\r\n",
    "\r\n",
    "### Setting up a Lambda function\r\n",
    "\r\n",
    "The first thing I'm going to do is set up a Lambda function. This Lambda function will be executed whenever our public API has data sent to it. When it is executed it will receive the data, perform any sort of processing that is required, send the data (the review) to the SageMaker endpoint I've created and then return the result.\r\n",
    "\r\n",
    "#### Part A: Creating an IAM Role for the Lambda function\r\n",
    "\r\n",
    "Since I want the Lambda function to call a SageMaker endpoint, I need to make sure that it has permission to do so. To do this, I will construct a role that I can later give the Lambda function.\r\n",
    "\r\n",
    "#### Part B: Creating a Lambda function\r\n",
    "\r\n",
    "Finally, I will create the lambda function with the following code.\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "```python\r\n",
    "# I need to use the low-level library to interact with SageMaker since the SageMaker API\r\n",
    "# is not available natively through Lambda.\r\n",
    "import boto3\r\n",
    "\r\n",
    "def lambda_handler(event, context):\r\n",
    "\r\n",
    "    # The SageMaker runtime is what allows us to invoke the endpoint that I've created.\r\n",
    "    runtime = boto3.Session().client('sagemaker-runtime')\r\n",
    "\r\n",
    "    # Now I'll use the SageMaker runtime to invoke our endpoint, sending the given review \r\n",
    "    response = runtime.invoke_endpoint(EndpointName = '**ENDPOINT NAME HERE**',    # The name of the endpoint I created\r\n",
    "                                       ContentType = 'text/plain',                 # The data format that is expected\r\n",
    "                                       Body = event['body'])                       # The actual review\r\n",
    "\r\n",
    "    # The response is an HTTP response whose body contains the result of our inference\r\n",
    "    result = response['Body'].read().decode('utf-8')\r\n",
    "\r\n",
    "    return {\r\n",
    "        'statusCode' : 200,\r\n",
    "        'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },\r\n",
    "        'body' : result\r\n",
    "    }\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# get the name of the endpoint\r\n",
    "predictor.endpoint"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'sagemaker-pytorch-2021-09-03-16-29-16-241'"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setting up API Gateway\r\n",
    "\r\n",
    "Now that the Lambda function is set up, The only thing that's left to do is to create a new API using API Gateway that will trigger the Lambda function I have just created.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Deploying the web app\r\n",
    "\r\n",
    "I created a simple web app for the deployment of the project.\\\r\n",
    "Now, wil try a positive and a negative review to make sure it's working properly.\r\n",
    "\r\n",
    "### **First, I will try it with a negative review:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Screenshot:**\r\n",
    "<img src=\"neg-review.png\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Then, I will try it with a positive review:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Screenshot:**\r\n",
    "<img src=\"pos-review.png\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# delete the endpoint\r\n",
    "predictor.delete_endpoint()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}